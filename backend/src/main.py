import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from data.adquisicion_datos import descargar_dataset_1, descargar_dataset_2
from preprocesamiento.preprocesamiento import procesar_imagenes
from feature_extraction.feature_processor import extraer_todas_las_caracteristicas
from cnn.embeddings import extraer_embeddings_cnn
from clustering.models import evaluar_todos_los_datasets, OnlineClusteringAPI

def verificar_caracteristicas_extraidas(out_dir):
    """
    Verifica si ya existen archivos CSV de caracterÃ­sticas para ambos datasets
    """
    archivos_esperados = [
        "momentos_clasicos.csv",
        "momentos_hu.csv", 
        "momentos_zernike.csv",
        "sift_features.csv",
        "hog_features.csv"
    ]
    
    # Verificar archivos para ambos datasets
    for dataset_num in [1, 2]:
        dataset_dir = os.path.join(out_dir, f"dataset_{dataset_num}")
        
        # Si el directorio del dataset no existe, faltan archivos
        if not os.path.exists(dataset_dir):
            return False
            
        # Verificar cada archivo CSV en el dataset
        for archivo in archivos_esperados:
            ruta_archivo = os.path.join(dataset_dir, archivo)
            if not os.path.exists(ruta_archivo):
                return False
    
    return True

def verificar_embeddings_cnn(out_dir):
    """
    Verifica si ya existen archivos de embeddings CNN para ambos datasets
    """
    archivos_esperados = [
        "Embeddings_cnn.npy",
        "Labels_cnn.npy"
    ]
    
    # Verificar archivos para ambos datasets
    for dataset_num in [1, 2]:
        dataset_dir = os.path.join(out_dir, f"dataset_{dataset_num}")
        
        # Si el directorio del dataset no existe, faltan archivos
        if not os.path.exists(dataset_dir):
            return False
            
        # Verificar cada archivo NPY en el dataset
        for archivo in archivos_esperados:
            ruta_archivo = os.path.join(dataset_dir, archivo)
            if not os.path.exists(ruta_archivo):
                return False
    
    return True

def verificar_resultados_clustering(resultados_dir):
    """
    Verifica si ya existen archivos de resultados de clustering
    """
    archivos_esperados = [
        "clustering_results_dataset_1.json",
        "clustering_results_dataset_2.json",
        "clustering_evaluation_results.csv"
    ]
    
    for archivo in archivos_esperados:
        ruta_archivo = os.path.join(resultados_dir, archivo)
        if not os.path.exists(ruta_archivo):
            return False
    return True

def verificar_imagenes_procesadas(carpeta_procesadas):
    """
    Verifica si ya existen imÃ¡genes procesadas en las subcarpetas
    Retorna True si ya hay imÃ¡genes procesadas
    """
    tipos = ["binaria", "contraste", "gris"]
    clases = ["0", "1", "2"]
    
    for tipo in tipos:
        for clase in clases:
            ruta_tipo_clase = os.path.join(carpeta_procesadas, tipo, clase)
            if os.path.exists(ruta_tipo_clase):
                # Verificar si tiene imÃ¡genes
                archivos = [f for f in os.listdir(ruta_tipo_clase) 
                           if f.lower().endswith((".png", ".jpg", ".jpeg", ".bmp", ".tif", ".tiff"))]
                if len(archivos) > 0:
                    return True
    return False

def preparar_rutas(dataset_num, dataset_func):
    """
    Devuelve un diccionario de rutas de cada clase dentro del dataset.
    Si ya existe la carpeta de crudas, no descarga.
    """
    carpeta_dataset = f"datasets/dataset_{dataset_num}"
    carpeta_crudas = os.path.join(carpeta_dataset, "imagenes_crudas")

    if not os.path.exists(carpeta_crudas):
        print(f"â¬‡ Descargando Dataset {dataset_num}...")
        rutas = dataset_func()
    else:
        print(f"âœ… Dataset {dataset_num} ya existe, no se descarga.")
        # Crear diccionario de clases basado en subcarpetas
        rutas = {clase: os.path.join(carpeta_crudas, clase)
                 for clase in os.listdir(carpeta_crudas)
                 if os.path.isdir(os.path.join(carpeta_crudas, clase))}

    # Asegurar que la carpeta de procesadas exista
    carpeta_procesadas = os.path.join(carpeta_dataset, "imagenes_procesadas")
    os.makedirs(carpeta_procesadas, exist_ok=True)

    return rutas, carpeta_procesadas

def api_demo_clustering_online(feature_dir, resultados_dir):
    """
    DemostraciÃ³n de la nueva API de clustering online
    """
    try:
        print("ğŸ”§ Inicializando API de Clustering Online...")
        api = OnlineClusteringAPI(feature_vectors_dir=feature_dir)
        
        # Mostrar configuraciÃ³n actual
        print("ğŸ“‹ ConfiguraciÃ³n actual:")
        params = api.get_hyperparameters()
        for key, value in params.items():
            print(f"   {key}: {value}")
        
        # Mostrar modelos disponibles
        print("\nğŸ“Š Modelos disponibles:")
        models = api.list_available_models()
        for model_id, info in models.items():
            print(f"   {model_id}: {info['name']}")
        
        # Prueba con un modelo rÃ¡pido (Momentos de Hu)
        print("\nğŸ§ª Ejecutando prueba con Momentos de Hu (Dataset 1)...")
        
        # Configurar parÃ¡metros optimizados
        api.update_hyperparameters(
            k=3,
            m=40,
            cluster_similarity_threshold=0.8,
            use_flexible=True
        )
        
        resultado = api.cluster(
            model_id='momentos_hu',
            dataset_num=1,
            use_flexible=True
        )
        
        if resultado['success']:
            print("âœ… Demo exitosa!")
            print(f"   Clusters formados: {resultado['clustering_results']['n_clusters_formed']}")
            print(f"   DistribuciÃ³n: {resultado['clustering_results']['cluster_counts']}")
            print(f"   ARI: {resultado['metrics']['external']['ARI']:.4f}")
            print(f"   NMI: {resultado['metrics']['external']['NMI']:.4f}")
            print(f"   Silhouette: {resultado['metrics']['internal']['Silhouette']:.4f}")
            
            # Guardar resultado de la demo
            api.save_results(resultado, resultados_dir)
            
        else:
            print(f"âŒ Error en demo: {resultado['error']}")
            
        print("ğŸ’¡ La API estÃ¡ lista para usar en src/api/app.py")
        
    except Exception as e:
        print(f"âš ï¸ Error en demo de API: {e}")
        print("ğŸ’¡ La evaluaciÃ³n tradicional se completÃ³ correctamente")

def main():
    print("=== Preparando datasets ===\n")

    # ------------------------------
    # Dataset 1
    # ------------------------------
    rutas1, carpeta_proc1 = preparar_rutas(1, descargar_dataset_1)
    print("\nDataset 1 listo:")
    for etiqueta, ruta in rutas1.items():
        print(f"Clase {etiqueta}: {ruta}")

    # ------------------------------
    # Dataset 2
    # ------------------------------
    rutas2, carpeta_proc2 = preparar_rutas(2, descargar_dataset_2)
    print("\nDataset 2 listo:")
    for etiqueta, ruta in rutas2.items():
        print(f"Clase {etiqueta}: {ruta}")

    # ------------------------------
    # Procesar imÃ¡genes
    # ------------------------------
    print("\n=== Procesando imÃ¡genes ===")
    for dataset_rutas, carpeta_procesadas, nombre_dataset in zip(
        [rutas1, rutas2],
        [carpeta_proc1, carpeta_proc2],
        ["dataset_1", "dataset_2"]
    ):
        # Verificar si ya existen imÃ¡genes procesadas
        if verificar_imagenes_procesadas(carpeta_procesadas):
            print(f"âœ… {nombre_dataset}: Las imÃ¡genes ya estÃ¡n procesadas, saltando procesamiento.")
            continue
            
        print(f"ğŸ”„ {nombre_dataset}: Procesando imÃ¡genes...")
        for etiqueta, carpeta_entrada in dataset_rutas.items():
            # Carpeta de salida dentro de imagenes_procesadas/tipo/clase
            carpeta_salida_gris = os.path.join(carpeta_procesadas, "gris", str(etiqueta))
            carpeta_salida_contraste = os.path.join(carpeta_procesadas, "contraste", str(etiqueta))
            carpeta_salida_binaria = os.path.join(carpeta_procesadas, "binaria", str(etiqueta))

            # Crear todas las carpetas necesarias
            os.makedirs(carpeta_salida_gris, exist_ok=True)
            os.makedirs(carpeta_salida_contraste, exist_ok=True)
            os.makedirs(carpeta_salida_binaria, exist_ok=True)

            # Procesar imÃ¡genes
            total = procesar_imagenes(
                carpeta_entrada,
                carpeta_salida_gris,
                carpeta_salida_contraste,
                carpeta_salida_binaria
            )
            print(f"[{nombre_dataset} Clase {etiqueta}] Procesadas {total} imÃ¡genes")

    print("\nâœ… Todos los datasets procesados y organizados correctamente.")

    # ------------------------------
    # ExtracciÃ³n de caracterÃ­sticas
    # ------------------------------
    print("\n=== Extrayendo caracterÃ­sticas ===")
    feature_dir = "src/feature_vectors"
    
    if verificar_caracteristicas_extraidas(feature_dir):
        print("âœ… Las caracterÃ­sticas ya estÃ¡n extraÃ­das para ambos datasets, saltando extracciÃ³n.")
        print(f"   Verificado: {feature_dir}/dataset_1/ y {feature_dir}/dataset_2/")
    else:
        print("ğŸ”„ Extrayendo caracterÃ­sticas para ambos datasets...")
        extraer_todas_las_caracteristicas(
            data_root="datasets",
            out_dir=feature_dir
        )

    # ------------------------------
    # ExtracciÃ³n de embeddings CNN
    # ------------------------------
    print("\n=== Extrayendo embeddings CNN ===")
    
    if verificar_embeddings_cnn(feature_dir):
        print("âœ… Los embeddings CNN ya estÃ¡n extraÃ­dos para ambos datasets, saltando extracciÃ³n.")
        print(f"   Verificado: {feature_dir}/dataset_1/ y {feature_dir}/dataset_2/")
    else:
        print("ğŸ”„ Extrayendo embeddings CNN para ambos datasets...")
        extraer_embeddings_cnn(
            data_root="datasets",
            out_dir=feature_dir
        )

    # ------------------------------
    # EvaluaciÃ³n con Clustering Online
    # ------------------------------
    print("\n=== Evaluando con Clustering Online ===")
    resultados_dir = "resultados"
    
    if verificar_resultados_clustering(resultados_dir):
        print("âœ… Los resultados de clustering ya estÃ¡n generados.")
        print("ğŸ’¡ Para forzar una nueva evaluaciÃ³n, elimina la carpeta 'resultados/'")
    else:
        print("ğŸ”„ Evaluando datasets con clustering online...")
        print("   MÃ©todo: LINKS-like con similitud coseno")
        print("   ParÃ¡metros: k=3 clusters, versiÃ³n flexible para clases desbalanceadas")
        
        # EvaluaciÃ³n completa con mÃ©todo tradicional
        evaluar_todos_los_datasets(
            k=3,  # NÃºmero de clusters (3 clases por dataset)
            feature_vectors_dir=feature_dir,
            resultados_dir=resultados_dir
        )
        
        print("âœ… EvaluaciÃ³n tradicional completada")
        
        # DemostraciÃ³n de la nueva API de clustering online
        print("\nğŸš€ Demostrando API de Clustering Online...")
        api_demo_clustering_online(feature_dir, resultados_dir)

    print("\n" + "="*60)
    print("ğŸ‰ PIPELINE COMPLETO EJECUTADO EXITOSAMENTE ğŸ‰")
    print("="*60)
    print(f"ğŸ“ Features y embeddings en: {feature_dir}")
    print(f"ğŸ“Š Resultados de clustering en: {resultados_dir}")
    print("ğŸ“ˆ Revisa los archivos de clustering para ver el rendimiento de cada mÃ©todo:")
    print(f"   - {resultados_dir}/clustering_results_dataset_1.json")
    print(f"   - {resultados_dir}/clustering_results_dataset_2.json") 
    print(f"   - {resultados_dir}/clustering_evaluation_results.csv")
    print("\nğŸ†• Archivos de la API Online:")
    print(f"   - {resultados_dir}/online_clustering_*.json")
    print("\nğŸŒ Para usar la API web, ejecuta: python src/api/app.py")

if __name__ == "__main__":
    main()
